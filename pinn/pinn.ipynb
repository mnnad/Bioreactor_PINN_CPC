{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mnnadi1\\Desktop\\PINN project\\Bioreactor_PINN_CPC\\pinn\\pinn.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mnnadi1/Desktop/PINN%20project/Bioreactor_PINN_CPC/pinn/pinn.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mnnadi1/Desktop/PINN%20project/Bioreactor_PINN_CPC/pinn/pinn.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mnnadi1/Desktop/PINN%20project/Bioreactor_PINN_CPC/pinn/pinn.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mparams\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparameters_for_BK\u001b[39;00m \u001b[39mimport\u001b[39;00m parameters_for_BK \u001b[39mas\u001b[39;00m pbk\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'params'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from params.parameters_for_BK import parameters_for_BK as pbk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    def __init__(self, \n",
    "                t_ini, x_ini, n_ini, q_ini, u_ini,  \n",
    "                f_in, f_out, width, depth, \n",
    "                w_init = \"Glorot\", b_init = \"zeros\", act = \"tanh\", \n",
    "                lr = 1e-3, opt = \"Adam\", \n",
    "                f_scl = \"minmax\", laaf = False, c = 1., \n",
    "                w_ini = 1., w_bnd = 1., w_pde = 1., BC = \"Neu\", \n",
    "                f_mntr = 10, r_seed = 1234):\n",
    "        self.mu_max = pbk.mu_max\n",
    "        self.K_S = pbk.K_S\n",
    "        self.P_crit = pbk.P_crit\n",
    "        # configuration\n",
    "        self.dat_typ = tf.float32\n",
    "        self.f_in    = f_in\n",
    "        self.f_out   = f_out\n",
    "        self.width   = width\n",
    "        self.depth   = depth\n",
    "        self.w_init  = w_init\n",
    "        self.b_init  = b_init\n",
    "        self.act     = act\n",
    "        self.lr      = lr\n",
    "        self.opt     = opt\n",
    "        self.f_scl   = f_scl\n",
    "        self.laaf    = laaf\n",
    "        self.c       = c\n",
    "        self.w_ini   = w_ini\n",
    "        self.w_bnd   = w_bnd\n",
    "        self.w_pde   = w_pde\n",
    "        self.BC      = BC\n",
    "        self.f_mntr  = f_mntr\n",
    "        self.r_seed  = r_seed\n",
    "        self.random_seed(self.r_seed)\n",
    "\n",
    "        # dataset\n",
    "        self.t_ini = t_ini; self.x_ini = x_ini; self.n_ini = n_ini; self.q_ini = q_ini; self.u_ini = u_ini\n",
    "\n",
    "\n",
    "        # bounds (for feature scaling)\n",
    "\n",
    "\n",
    "\n",
    "        # build\n",
    "        self.structure = [self.f_in] + (self.depth-1) * [self.width] + [self.f_out]\n",
    "        self.weights, self.biases, self.alphas, self.params = self.dnn_init(self.structure)\n",
    "\n",
    "        # system param\n",
    "        self.c = tf.constant(self.c, dtype = self.dat_typ)\n",
    "\n",
    "        # optimization\n",
    "        self.optimizer = self.opt_(self.lr, self.opt)\n",
    "        self.ep_log       = []\n",
    "        self.loss_log     = []\n",
    "        self.loss_ini_log = []\n",
    "        self.loss_bnd_log = []\n",
    "        self.loss_pde_log = []\n",
    "\n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"****************     MAIN PROGRAM START     ****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> start time:\", datetime.datetime.now())\n",
    "        print(\">>>>> configuration;\")\n",
    "        print(\"         random seed  :\", self.r_seed)\n",
    "        print(\"         data type    :\", self.dat_typ)\n",
    "        print(\"         activation   :\", self.act)\n",
    "        print(\"         weight init  :\", self.w_init)\n",
    "        print(\"         bias   init  :\", self.b_init)\n",
    "        print(\"         learning rate:\", self.lr)\n",
    "        print(\"         optimizer    :\", self.opt)\n",
    "        print(\"         width        :\", self.width)\n",
    "        print(\"         depth        :\", self.depth)\n",
    "        print(\"         structure    :\", self.structure)\n",
    "\n",
    "    def random_seed(self, seed = 1234):\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "    def dnn_init(self, strc):\n",
    "        weights = []\n",
    "        biases  = []\n",
    "        alphas  = []\n",
    "        params  = []\n",
    "        for d in range(0, self.depth):   # depth = self.depth\n",
    "            w = self.weight_init(shape = [strc[d], strc[d + 1]], depth = d)\n",
    "            b = self.bias_init  (shape = [      1, strc[d + 1]], depth = d)\n",
    "            weights.append(w)\n",
    "            biases .append(b)\n",
    "            params .append(w)\n",
    "            params .append(b)\n",
    "            if self.laaf == True and d < self.depth - 1:\n",
    "                a = tf.Variable(1., dtype = self.dat_typ, name = \"a\" + str(d))\n",
    "                params.append(a)\n",
    "            else:\n",
    "                a = tf.constant(1., dtype = self.dat_typ)\n",
    "            alphas .append(a)\n",
    "        return weights, biases, alphas, params\n",
    "\n",
    "    def weight_init(self, shape, depth):\n",
    "        in_dim  = shape[0]\n",
    "        out_dim = shape[1]\n",
    "        if self.w_init == \"Glorot\":\n",
    "            std = np.sqrt(2 / (in_dim + out_dim))\n",
    "        elif self.w_init == \"He\":\n",
    "            std = np.sqrt(2 / in_dim)\n",
    "        elif self.w_init == \"LeCun\":\n",
    "            std = np.sqrt(1 / in_dim)\n",
    "        else:\n",
    "            raise NotImplementedError(\">>>>> weight_init\")\n",
    "        weight = tf.Variable(\n",
    "            tf.random.truncated_normal(shape = [in_dim, out_dim], \\\n",
    "            mean = 0., stddev = std, dtype = self.dat_typ), \\\n",
    "            dtype = self.dat_typ, name = \"w\" + str(depth)\n",
    "            )\n",
    "        return weight\n",
    "\n",
    "    def bias_init(self, shape, depth):\n",
    "        in_dim  = shape[0]\n",
    "        out_dim = shape[1]\n",
    "        if self.b_init == \"zeros\":\n",
    "            bias = tf.Variable(\n",
    "                tf.zeros(shape = [in_dim, out_dim], dtype = self.dat_typ), \\\n",
    "                dtype = self.dat_typ, name = \"b\" + str(depth)\n",
    "                )\n",
    "        elif self.b_init == \"ones\":\n",
    "            bias = tf.Variable(\n",
    "                tf.ones(shape = [in_dim, out_dim], dtype = self.dat_typ), \\\n",
    "                dtype = self.dat_typ, name = \"b\" + str(depth)\n",
    "                )\n",
    "        else:\n",
    "            raise NotImplementedError(\">>>>> bias_init\")\n",
    "        return bias\n",
    "\n",
    "    def opt_(self, lr, opt):\n",
    "        if opt == \"SGD\":\n",
    "            optimizer = tf.keras.optimizers.SGD(\n",
    "                learning_rate = lr, momentum = 0.0, nesterov = False\n",
    "                )\n",
    "        elif opt == \"RMSprop\":\n",
    "            optimizer = tf.keras.optimizers.RMSprop(\n",
    "                learning_rate = lr, rho = 0.9, momentum = 0.0, centered = False\n",
    "                )\n",
    "        elif opt == \"Adam\":\n",
    "            optimizer = tf.keras.optimizers.Adam(\n",
    "                learning_rate = lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False\n",
    "                )\n",
    "        elif opt == \"Adamax\":\n",
    "            optimizer = tf.keras.optimizers.Adamax(\n",
    "                learning_rate = lr, beta_1 = 0.9, beta_2 = 0.999\n",
    "                )\n",
    "        elif opt == \"Nadam\":\n",
    "            optimizer = tf.keras.optimizers.Nadam(\n",
    "                learning_rate = lr, beta_1 = 0.9, beta_2 = 0.999\n",
    "                )\n",
    "        else:\n",
    "            raise NotImplementedError(\">>>>> opt_\")\n",
    "        return optimizer\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        z = x\n",
    "        for d in range(0, self.depth - 1):\n",
    "            w = self.weights[d]\n",
    "            b = self.biases [d]\n",
    "            a = self.alphas [d]\n",
    "            u = tf.add(tf.matmul(z, w), b)\n",
    "            u = tf.multiply(a, u)\n",
    "            if self.act == \"tanh\":\n",
    "                z = tf.tanh(u)\n",
    "            elif self.act == \"swish\":\n",
    "                z = tf.multiply(u, tf.sigmoid(u))\n",
    "            elif self.act == \"gelu\":\n",
    "                z = tf.multiply(u, tf.sigmoid(1.702 * u))\n",
    "            elif self.act == \"mish\":\n",
    "                z = tf.multiply(u, tf.tanh(tf.nn.softplus(u)))\n",
    "            else:\n",
    "                raise NotImplementedError(\">>>>> forward_pass (act)\")\n",
    "        w = self.weights[-1]\n",
    "        b = self.biases [-1]\n",
    "        a = self.alphas [-1]\n",
    "        u = tf.add(tf.matmul(z, w), b)\n",
    "        u = tf.multiply(a, u)\n",
    "        z = u   # identity mapping\n",
    "\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def pde(self, t, x, n, q):\n",
    "        t = tf.convert_to_tensor(t, dtype = self.dat_typ)\n",
    "        x = tf.convert_to_tensor(x, dtype = self.dat_typ)\n",
    "        n = tf.convert_to_tensor(n, dtype = self.dat_typ)\n",
    "        q = tf.convert_to_tensor(q, dtype = self.dat_typ)      \n",
    "        with tf.GradientTape(persistent = True) as tp:\n",
    "            tp.watch(t)\n",
    "            tp.watch(x)\n",
    "            tp.watch(n)\n",
    "            tp.watch(q)            \n",
    "            \n",
    "            u_v_p = self.forward_pass(tf.concat([t, x, n, q], 1))\n",
    "            \n",
    "            x_1 = u_v_p[:,1:2] \n",
    "            n_1 = u_v_p[:,2:3] \n",
    "            q_1 = u_v_p[:,3:4]             \n",
    "            \n",
    "            \n",
    "            x_t = tp.gradient(x_1, t)\n",
    "\n",
    "\n",
    "        del tp\n",
    "               \n",
    "########################################################################################\n",
    "\n",
    "        self.mu_max = pbk.mu_max\n",
    "        self.K_S = pbk.K_S\n",
    "        self.P_crit = pbk.P_crit        \n",
    "        \n",
    "        gt = x_t + self.mu_max * (n_1 / (self.K_S + n_1)) * x_1 * (q_1/self.P_crit) * (1- q_1/self.P_crit)\n",
    "\n",
    "   \n",
    "###########################################################################################        \n",
    "\n",
    "        out = u_v_p       \n",
    "        return out, gt\n",
    "\n",
    "\n",
    "    def loss_ini(self, t, x, n, q, data_in):\n",
    "\n",
    "        u_, gt_ = self.pde(t, x, n, q)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(data_in - u_)) + tf.reduce_mean(tf.square(gt_)) \n",
    "                \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def loss_grad(self, t, x, n, q, data_in):\n",
    "        with tf.GradientTape(persistent=True) as tp:\n",
    "            loss = self.loss_ini(t, x, n, q, data_in)\n",
    "        grad = tp.gradient(loss, self.params)\n",
    "        del tp\n",
    "        return loss, grad\n",
    "\n",
    "\n",
    "    \n",
    "    def grad_desc(self, t, x, n, q, data_in):\n",
    "        loss, grad = self.loss_grad(t, x, n, q, data_in)\n",
    "        self.optimizer.apply_gradients(zip(grad, self.params))\n",
    "        return loss\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, \n",
    "            epoch = 10 ** 5, batch = 2 ** 6, tol = 1e-5): \n",
    "        print(\">>>>> training setting;\")\n",
    "        print(\"         # of epoch     :\", epoch)\n",
    "        print(\"         batch size     :\", batch)\n",
    "        print(\"         convergence tol:\", tol)\n",
    "        \n",
    "        t_ini = self.t_ini; x_ini = self.x_ini; n_ini = self.n_ini; \n",
    "        q_ini = self.q_ini; data_ini = self.u_ini\n",
    "\n",
    "        \n",
    "        t0 = time.time()\n",
    "        for ep in range(epoch):\n",
    "            es_pat      = 0\n",
    "            es_crt      = 5\n",
    "            min_loss    = 100.\n",
    "            ep_loss     = 0.\n",
    "            ep_loss_ini = 0.\n",
    "            ep_loss_bnd = 0.\n",
    "            ep_loss_pde = 0.\n",
    "            \n",
    "            # full-batch training\n",
    "            if batch == 0:\n",
    "                ep_loss = self.grad_desc(t_ini, x_ini, n_ini, q_ini, data_ini)\n",
    "\n",
    "            \n",
    "            # mini-batch training\n",
    "            else:\n",
    "                bound_b = self.x_ini.shape[0]\n",
    "                idx_b = np.random.permutation(bound_b)\n",
    "\n",
    "                for idx in range(0, bound_b, batch):\n",
    "                    # batch for initial condition\n",
    "                    t_ini_b = tf.convert_to_tensor(t_ini.numpy()[idx_b[idx:idx+batch if idx+batch < bound_b else bound_b]], dtype = self.dat_typ)\n",
    "                    x_ini_b = tf.convert_to_tensor(x_ini.numpy()[idx_b[idx:idx+batch if idx+batch < bound_b else bound_b]], dtype = self.dat_typ)\n",
    "                    n_ini_b = tf.convert_to_tensor(n_ini.numpy()[idx_b[idx:idx+batch if idx+batch < bound_b else bound_b]], dtype = self.dat_typ)\n",
    "                    q_ini_b = tf.convert_to_tensor(q_ini.numpy()[idx_b[idx:idx+batch if idx+batch < bound_b else bound_b]], dtype = self.dat_typ)\n",
    "                    data_ini_b = tf.convert_to_tensor(data_ini.numpy()[idx_b[idx:idx+batch if idx+batch < bound_b else bound_b]], dtype = self.dat_typ)\n",
    "                   \n",
    "                    # compute loss and perform gradient descent\n",
    "                    loss_b = self.grad_desc(t_ini_b, x_ini_b, n_ini_b, q_ini_b, data_ini_b)\n",
    "                                        \n",
    "\n",
    "                    # per batch -> per epoch\n",
    "                    ep_loss     += loss_b     / int(bound_b / batch)\n",
    "\n",
    "                        \n",
    "            if ep % self.f_mntr == 0:\n",
    "                elps = time.time() - t0\n",
    "                self.ep_log.append(ep)\n",
    "                self.loss_log.append(ep_loss)\n",
    "                \n",
    "                self.loss_ini_log.append(ep_loss_ini)\n",
    "                self.loss_bnd_log.append(ep_loss_bnd)\n",
    "                self.loss_pde_log.append(ep_loss_pde)\n",
    "                print(\"ep: %d, loss: %.3e, elps: %.3f\" \n",
    "                    % (ep, ep_loss, elps))\n",
    "                t0 = time.time()\n",
    "            \n",
    "                if ep_loss < min_loss:\n",
    "                    es_pat = 0\n",
    "                    min_loss = ep_loss\n",
    "                else:\n",
    "                    es_pat += 1\n",
    "                    print(\">>>>> observed loss increase, patience: %d\" % es_pat)\n",
    "\n",
    "            if ep_loss < tol:\n",
    "                print(\">>>>> program terminating with the loss converging to its tolerance.\")\n",
    "                print(\"\\n************************************************************\")\n",
    "                print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "                print(\"************************************************************\")\n",
    "                print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                break\n",
    "            elif es_crt < es_pat:\n",
    "                print(\">>>>> program terminating with early stopping triggered.\")\n",
    "                print(\"\\n************************************************************\")\n",
    "                print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "                print(\"************************************************************\")\n",
    "                print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        print(\"\\n************************************************************\")\n",
    "        print(\"*****************     MAIN PROGRAM END     *****************\")\n",
    "        print(\"************************************************************\")\n",
    "        print(\">>>>> end time:\", datetime.datetime.now())\n",
    "                \n",
    "    def infer(self, t, x, y):\n",
    "        u_, gv_ = self.pde(t, x, y)\n",
    "        return u_, gv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transferlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
